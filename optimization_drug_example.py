# drug efficacy optimization example for BME 2315
# made by Lavie, fall 2025

#%% import libraries
import numpy as np
import matplotlib.pyplot as plt


#%% define drug models

# define toxicity levels for each drug (lambda)
metformin_lambda = 0.5

lisinopril_lambda = 0.8

escitalopram_lambda = 0.3

def metformin(x):   # mild toxicity, moderate efficacy
    efficacy = 0.8 * np.exp(-0.1*(x-5)**2)
    toxicity = 0.2 * x**2 / 100
    return efficacy - metformin_lambda * toxicity
def lisinopril(x):  # strong efficacy, higher toxicity
    efficacy = np.exp(-0.1*(x-7)**2)
    toxicity = 0.3 * x**2 / 80
    return efficacy - lisinopril_lambda * toxicity
def escitalopram(x):  # weaker efficacy, low toxicity
    efficacy = 0.6 * np.exp(-0.1*(x-4)**2)
    toxicity = 0.1 * x**2 / 120
    return efficacy - escitalopram_lambda * toxicity

#%% plot drug efficacies
x = np.linspace(0, 15, 100)
fig, ax = plt.subplots(figsize=(10, 6))
plt.plot(x, metformin(x), label='Metformin', color='blue')
plt.plot(x, lisinopril(x), label='Lisinopril', color='orange')
plt.plot(x, escitalopram(x), label='Escitalopram', color='green')
plt.title('Drug Efficacy vs Dosage')
plt.xlabel('Dosage (mg)')
plt.ylabel('Net Effect')
plt.legend()

# %% Find optimal dosages for each drug

# First method: Steepest Ascent using the update rule

# first, need the first derivative (gradient)
def gradient(f, x, h=1e-4):
    """Central difference approximation for f'(x)."""
    return (f(x + h) - f(x - h)) / (2*h)

def steepest_ascent(f, x0, h_step=0.1, tol=1e-6, max_iter=1000):
    x = x0 # update initial guess
    for i in range(max_iter):
        grad = gradient(f, x)
        x_new = x + h_step * grad     
        
        if abs(x_new - x) < tol:      # convergence condition, when solution is 0
            print(f"Converged in {i+1} iterations.")
            break
            
        x = x_new
    return x, f(x)

# metformin
opt_dose_metformin, opt_effect_metformin = steepest_ascent(metformin, x0=1.0)
print(f"Steepest Ascent Method - Optimal Metformin Dose: {opt_dose_metformin:.2f} mg")
print(f"Steepest Ascent Method - Optimal Metformin Effect: {opt_effect_metformin*100:.2f}%")

# lisinopril
opt_dose_lisinopril, opt_effect_lisinopril = steepest_ascent(lisinopril, x0=1.0)
print(f"Steepest Ascent Method - Optimal Lisinopril Dose: {opt_dose_lisinopril:.2f} mg")
print(f"Steepest Ascent Method - Optimal Lisinopril Effect: {opt_effect_lisinopril*100:.2f}%")

# escitalopram
opt_dose_escitalopram, opt_effect_escitalopram = steepest_ascent(escitalopram, x0=1.0)
print(f"Steepest Ascent Method - Optimal Escitalopram Dose: {opt_dose_escitalopram:.2f} mg")
print(f"Steepest Ascent Method - Optimal Escitalopram Effect: {opt_effect_escitalopram*100:.2f}%")

# %% Newton's method

# requires second derivative
def second_derivative(f, x, h=1e-4):
    """Central difference approximation for f''(x)."""
    return (f(x + h) - 2*f(x) + f(x - h)) / (h**2)

def newtons_method(f, x0, tol=1e-6, max_iter=1000):
    x = x0
    for i in range(max_iter):
        grad = gradient(f, x)
        hess = second_derivative(f, x)
        
        if hess == 0:  # avoid division by zero
            print("Zero second derivative. No solution found.")
            return x, f(x)
        
        x_new = x - grad / hess
        
        if abs(x_new - x) < tol:
            print(f"Converged in {i+1} iterations.")
            break
            
        x = x_new
    return x, f(x)

# metformin
opt_dose_metformin_nm, opt_effect_metformin_nm = newtons_method(metformin, x0=1.0)
print(f"Newton's Method - Optimal Metformin Dose: {opt_dose_metformin_nm:.2f} mg")
print(f"Newton's Method - Optimal Metformin Effect: {opt_effect_metformin_nm*100:.2f}%")                

# lisinopril
opt_dose_lisinopril_nm, opt_effect_lisinopril_nm = newtons_method(lisinopril, x0=1.0)
print(f"Newton's Method - Optimal Lisinopril Dose: {opt_dose_lisinopril_nm:.2f} mg")
print(f"Newton's Method - Optimal Lisinopril Effect: {opt_effect_lisinopril_nm*100:.2f}%")

# escitalopram
opt_dose_escitalopram_nm, opt_effect_escitalopram_nm = newtons_method(escitalopram, x0=1.0)
print(f"Newton's Method - Optimal Escitalopram Dose: {opt_dose_escitalopram_nm:.2f} mg")
print(f"Newton's Method - Optimal Escitalopram Effect: {opt_effect_escitalopram_nm*100:.2f}%")

#1) If you pick a higher lambda  (more toxicity), the optimal dose will likely decrease, as the model will penalize higher doses more heavily due to increased toxicity. Conversely, if you pick a lower lambda (less toxicity), the optimal dose may increase, as the model will be more tolerant of higher doses that may have greater efficacy.

#2) Applying steepest ascent vs Newton's method, Newton's method converged in less iterations than steepest ascent for all three drugs. This is because Newton's method uses second-order information (the Hessian) to make more informed updates, while steepest ascent only uses first-order information (the gradient), which can lead to slower convergence, especially near the optimal point.

#3) If we change the max_iter (# of iteration) the optimal value would change because if the max_iter is too low, the algorithm may not have enough iterations to converge to the optimal solution, resulting in a suboptimal dose. On the other hand, if max_iter is set too high, it may lead to unnecessary computations without significant improvement in the solution, especially if the algorithm has already converged. Therefore, it's important to choose an appropriate max_iter value based on the expected convergence behavior of the optimization problem.


#%% Here is the combined effect (Metformin + Lisinopril + Escitalopram)
def combined_net_effect(x):
    return metformin(x) + lisinopril(x) + escitalopram(x)

# here we plot combined effect on the same graph
plt.plot(x, combined_net_effect(x), label='Combined', color='black', linestyle='--')
plt.legend()

# I optimized combined effect with both methods
opt_dose_combined_sa, opt_effect_combined_sa = steepest_ascent(combined_net_effect, x0=1.0)
print(f"Steepest Ascent Method - Optimal Combined Dose: {opt_dose_combined_sa:.2f} mg")
print(f"Steepest Ascent Method - Optimal Combined Effect: {opt_effect_combined_sa*100:.2f}%")

opt_dose_combined_nm, opt_effect_combined_nm = newtons_method(combined_net_effect, x0=1.0)
print(f"Newton's Method - Optimal Combined Dose: {opt_dose_combined_nm:.2f} mg")    
print(f"Newton's Method - Optimal Combined Effect: {opt_effect_combined_nm*100:.2f}%")

# I chose one drug to tune lambda (Metformin) to match the combined optimal dose
def metformin_with_lambda(x, lam):
    efficacy = 0.8 * np.exp(-0.1 * (x - 5) ** 2)
    toxicity = 0.2 * x**2 / 100
    return efficacy - lam * toxicity

target_dose = opt_dose_combined_nm
lambda_values = np.linspace(0.1, 1.5, 15)
best_lambda = None
best_dose = None
best_error = np.inf

for lam in lambda_values:
    f = lambda x, lam=lam: metformin_with_lambda(x, lam)
    dose_hat, _ = newtons_method(f, x0=1.0)
    error = abs(dose_hat - target_dose)
    if error < best_error:
        best_error = error
        best_lambda = lam
        best_dose = dose_hat

print(f"Best Metformin lambda to match combined dose: {best_lambda:.2f}")
print(f"Metformin dose at best lambda: {best_dose:.2f} mg (target {target_dose:.2f} mg)")

# I am done now with the plotting here
